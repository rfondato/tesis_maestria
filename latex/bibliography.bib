@inproceedings{Ann2014,
abstract = {Human Activity Recognition is one of the active research areas in computer vision for various contexts like security surveillance, healthcare and human computer interaction. In this paper, a total of thirty-two recent research papers on sensing technologies used in HAR are reviewed. The review covers three area of sensing technologies namely RGB cameras, depth sensors and wearable devices. It also discusses on the pros and cons of the mentioned sensing technologies. The findings showed that RGB cameras have lower popularity when compared to depth sensors and wearable devices in HAR research.},
author = {Ann, Ong Chin and Theng, Lau Bee},
booktitle = {Proceedings - 4th IEEE International Conference on Control System, Computing and Engineering, ICCSCE 2014},
doi = {10.1109/ICCSCE.2014.7072750},
file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ann, on, 2014 - Unknown - Human activity recognition A review.pdf:pdf},
isbn = {9781479956869},
keywords = {Human activity recognition,Kinect,RGB camera,depth sensor,sensing technology,wearable devices},
pages = {389--393},
title = {{Human activity recognition: A review}},
url = {https://ieeexplore.ieee.org/abstract/document/7072750/},
year = {2014}
}
@inproceedings{Asadi-Aghbolaghi2017,
abstract = {The interest in action and gesture recognition has grown considerably in the last years. In this paper, we present a survey on current deep learning methodologies for action and gesture recognition in image sequences. We introduce a taxonomy that summarizes important aspects of deep learning for approaching both tasks. We review the details of the proposed architectures, fusion strategies, main datasets, and competitions. We summarize and discuss the main works proposed so far with particular interest on how they treat the temporal dimension of data, discussing their main features and identify opportunities and challenges for future research.},
author = {Asadi-Aghbolaghi, Maryam and Clapes, Albert and Bellantonio, Marco and Escalante, Hugo Jair and Ponce-Lopez, Victor and Baro, Xavier and Guyon, Isabelle and Kasaei, Shohreh and Escalera, Sergio},
booktitle = {Proceedings - 12th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2017 - 1st International Workshop on Adaptive Shot Learning for Gesture Understanding and Production, ASL4GUP 2017, Biometrics in the Wild, Bwild 2017, Heteroge},
doi = {10.1109/FG.2017.150},
file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Asadi-Aghbolaghi et al. - Unknown - A survey on deep learning based approaches for action and gesture recognition in image sequences.pdf:pdf},
isbn = {9781509040230},
pages = {476--483},
title = {{A Survey on Deep Learning Based Approaches for Action and Gesture Recognition in Image Sequences}},
url = {https://ieeexplore.ieee.org/abstract/document/7961779/},
year = {2017}
}
@article{Beauchemin1995,
author = {Beauchemin, S. S. and Barron, J. L.},
doi = {10.1145/212094.212141},
file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Beauchemin, Barron - 1995 - The Computation of Optical Flow.pdf:pdf},
issn = {15577341},
journal = {ACM Computing Surveys (CSUR)},
month = {sep},
number = {3},
pages = {433--466},
title = {{The Computation of Optical Flow}},
volume = {27},
year = {1995}
}
@article{Beddiar2020,
abstract = {Human activity recognition (HAR) systems attempt to automatically identify and analyze human activities using acquired information from various types of sensors. Although several extensive review papers have already been published in the general HAR topics, the growing technologies in the field as well as the multi-disciplinary nature of HAR prompt the need for constant updates in the field. In this respect, this paper attempts to review and summarize the progress of HAR systems from the computer vision perspective. Indeed, most computer vision applications such as human computer interaction, virtual reality, security, video surveillance and home monitoring are highly correlated to HAR tasks. This establishes new trend and milestone in the development cycle of HAR systems. Therefore, the current survey aims to provide the reader with an up to date analysis of vision-based HAR related literature and recent progress in the field. At the same time, it will highlight the main challenges and future directions.},
author = {Beddiar, Djamila Romaissa and Nini, Brahim and Sabokrou, Mohammad and Hadid, Abdenour},
doi = {10.1007/s11042-020-09004-3},
file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Beddiar et al. - 2020 - Vision-based human activity recognition a survey.pdf:pdf},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {Action detection,Action representation,Behavior understanding,Computer Vision,Human activity recognition,Survey},
month = {nov},
number = {41-42},
pages = {30509--30555},
publisher = {Springer},
title = {{Vision-based human activity recognition: a survey}},
volume = {79},
year = {2020}
}
@inproceedings{Bourdev2009,
abstract = {We address the classic problems of detection, segmentation and pose estimation of people in images with a novel definition of a part, a poselet. We postulate two criteria (1) It should be easy to find a poselet given an input image (2) it should be easy to localize the 3D configuration of the person conditioned on the detection of a poselet. To permit this we have built a new dataset, H3D, of annotations of humans in 2D photographs with 3D joint information, inferred using anthropometric constraints. This enables us to implement a data-driven search procedure for finding poselets that are tightly clustered in both 3D joint configuration space as well as 2D image appearance. The algorithm discovers poselets that correspond to frontal and profile faces, pedestrians, head and shoulder views, among others. Each poselet provides examples for training a linear SVM classifier which can then be run over the image in a multiscale scanning mode. The outputs of these poselet detectors can be thought of as an intermediate layer of nodes, on top of which one can run a second layer of classification or regression. We show how this permits detection and localization of torsos or keypoints such as left shoulder, nose, etc. Experimental results show that we obtain state of the art performance on people detection in the PASCAL VOC 2007 challenge, among other datasets. We are making publicly available both the H3D dataset as well as the poselet parameters for use by other researchers.},
author = {Bourdev, Lubomir and Malik, Jitendra},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2009.5459303},
file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bourdev, conference, 2009 - Unknown - Poselets Body part detectors trained using 3d human pose annotations.pdf:pdf},
issn = {15505499},
pages = {1365--1372},
title = {{Poselets: Body Part Detectors Trained Using 3D Human Pose Annotations}},
url = {https://ieeexplore.ieee.org/abstract/document/5459303/},
year = {2009}
}
@inproceedings{Chaudhry2009,
abstract = {System theoretic approaches to action recognition model the dynamics of a scene with linear dynamical systems (LDSs) and perform classification using metrics on the space of LDSs, e.g. Binet-Cauchy kernels. However, such approaches are only applicable to time series data living in a Euclidean space, e.g. joint trajectories extracted from motion capture data or feature point trajectories extracted from video. Much of the success of recent object recognition techniques relies on the use of more complex feature descriptors, such as SIFT descriptors or HOG descriptors, which are essentially histograms. Since histograms live in a non-Euclidean space, we can no longer model their temporal evolution with LDSs, nor can we classify them using a metric for LDSs. In this paper, we propose to represent each frame of a video using a histogram of oriented optical flow (HOOF) and to recognize human actions by classifying HOOF time-series. For this purpose, we propose a generalization of the Binet-Cauchy kernels to nonlinear dynamical systems (NLDS) whose output lives in a non-Euclidean space, e.g. the space of histograms. This can be achieved by using kernels defined on the original non-Euclidean space, leading to a well-defined metric for NLDSs. We use these kernels for the classification of actions in video sequences using (HOOF) as the output of the NLDS. We evaluate our approach to recognition of human actions in several scenarios and achieve encouraging results. {\textcopyright}2009 IEEE.},
author = {Chaudhry, Rizwan and Ravichandran, Avinash and Hager, Gregory and Vidal, Ren{\'{e}}},
booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2009},
doi = {10.1109/CVPRW.2009.5206821},
file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chaudhry et al. - 2009 - Histograms of oriented optical flow and binet-cauchy kernels on nonlinear dynamical systems for the recognition.pdf:pdf},
isbn = {9781424439935},
pages = {1932--1939},
title = {{Histograms of oriented optical flow and Binet-Cauchy kernels on nonlinear dynamical systems for the recognition of human actions}},
url = {https://ieeexplore.ieee.org/abstract/document/5206821/},
year = {2009}
}
@inproceedings{Dalal2005,
abstract = {We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds. {\textcopyright} 2005 IEEE.},
author = {Dalal, Navneet and Triggs, Bill},
booktitle = {Proceedings - 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, CVPR 2005},
doi = {10.1109/CVPR.2005.177},
file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dalal, on, 2005 - 2005 - Histograms of oriented gradients for human detection.pdf:pdf},
isbn = {0769523722},
pages = {886--893},
title = {{Histograms of oriented gradients for human detection}},
url = {https://ieeexplore.ieee.org/abstract/document/1467360/},
volume = {I},
year = {2005}
}
@inproceedings{Dollar2005,
abstract = {A common trend in object recognition is to detect and lever-age the use of sparse, informative feature points, The use of such features makes the problem more manageable while providing increased robustness to noise and pose variation. In this work we develop an extension of these ideas to the spatio-temporal case. For this purpose, we show that the direct 3D counterparts to commonly used 2D interest point detectors are inadequate, and we propose an alternative. Anchoring off of these interest points, we devise a recognition algorithm based on spatio-temporally windowed data. We present recognition results on a variety of datasets including both human and rodent behavior. {\textcopyright}2005 IEEE.},
author = {Doll{\'{a}}r, Piotr and Rabaud, Vincent and Cottrell, Garrison and Belongie, Serge},
booktitle = {Proceedings - 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance, VS-PETS},
doi = {10.1109/VSPETS.2005.1570899},
file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Doll{\'{a}}r et al. - Unknown - Behavior recognition via sparse spatio-temporal features(2).pdf:pdf},
isbn = {0780394240},
pages = {65--72},
title = {{Behavior recognition via sparse spatio-temporal features}},
url = {https://ieeexplore.ieee.org/abstract/document/1570899/},
volume = {2005},
year = {2005}
}
@inproceedings{Efros2003,
	title={Recognizing action at a distance},
	author={Efros and Berg and Mori and Malik},
	booktitle={Proceedings Ninth IEEE International Conference on Computer Vision},
	pages={726--733},
	year={2003},
	organization={IEEE}
}
@inproceedings{Gerling2012,
abstract = {Older adults in nursing homes often lead sedentary lifestyles, which reduces their life expectancy. Full-body motion-control games provide an opportunity for these adults to remain active and engaged; these games are not designed with age-related impairments in mind, which prevents the games from being leveraged to increase the activity levels of older adults. In this paper, we present two studies aimed at developing game design guidelines for full-body motion controls for older adults experiencing age-related changes and impairments. Our studies also demonstrate how full-body motion-control games can accommodate a variety of user abilities, have a positive effect on mood and, by extension, the emotional well-being of older adults. Based on our studies, we present seven guidelines for the design of full-body interaction in games. The guidelines are designed to foster safe physical activity among older adults, thereby increasing their quality of life. Copyright 2012 ACM.},
author = {Gerling, Kathrin M. and Livingston, Ian J. and Nacke, Lennart E. and Mandryk, Regan L.},
booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
doi = {10.1145/2207676.2208324},
file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gerling et al. - 2012 - Full-body motion-based game interaction for older adults.pdf:pdf},
isbn = {9781450310154},
keywords = {Design,Entertainment,Games,Older adults},
pages = {1873--1882},
title = {{Full-body motion-based game interaction for older adults}},
url = {https://dl.acm.org/doi/abs/10.1145/2207676.2208324},
year = {2012}
}
@article{Gonzalez-Ortega2014,
abstract = {In this paper, a 3D computer vision system for cognitive assessment and rehabilitation based on the Kinect device is presented. It is intended for individuals with body scheme dysfunctions and left-right confusion. The system processes depth information to overcome the shortcomings of a previously presented 2D vision system for the same application. It achieves left and right-hand tracking, and face and facial feature detection (eye, nose, and ears) detection. The system is easily implemented with a consumer-grade computer and an affordable Kinect device and is robust to drastic background and illumination changes. The system was tested and achieved a successful monitoring percentage of 96.28%. The automation of the human body parts motion monitoring, its analysis in relation to the psychomotor exercise indicated to the patient, and the storage of the result of the realization of a set of exercises free the rehabilitation experts of doing such demanding tasks. The vision-based system is potentially applicable to other tasks with minor changes. {\textcopyright} 2013 Elsevier Ireland Ltd.},
author = {Gonz{\'{a}}lez-Ortega, D and D{\'{i}}az-Pernas, F J and Mart{\'{i}}nez-Zarzuela, M and Ant{\'{o}}n-Rodr{\'{i}}guez, M},
doi = {10.1016/j.cmpb.2013.10.014},
file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gonz{\'{a}}lez-Ortega et al. - 2013 - A Kinect-based system for cognitive rehabilitation exercises monitoring.pdf:pdf},
issn = {01692607},
journal = {Computer Methods and Programs in Biomedicine},
keywords = {3D computer vision,Cognitive rehabilitation,Human body parts detection and tracking,Human-computer interaction,Kinect device},
number = {2},
pages = {620--631},
pmid = {24263055},
title = {{A Kinect-based system for cognitive rehabilitation exercises monitoring}},
url = {https://www.sciencedirect.com/science/article/pii/S0169260713003568},
volume = {113},
year = {2014}
}
@article{Gorelick2007,
	title={Actions as space-time shapes},
	author={Gorelick, Lena and Blank, Moshe and Shechtman, Eli and Irani, Michal and Basri, Ronen},
	journal={IEEE transactions on pattern analysis and machine intelligence},
	volume={29},
	number={12},
	pages={2247--2253},
	year={2007},
	publisher={IEEE}
}
@inproceedings{Jhuang2007,
	title={A biologically inspired system for action recognition},
	author={Jhuang, Hueihan and Serre, Thomas and Wolf, Lior and Poggio, Tomaso},
	booktitle={2007 IEEE 11th international conference on computer vision},
	pages={1--8},
	year={2007},
	organization={Ieee}
}
@article{Ji2013,
abstract = {We consider the automated recognition of human actions in surveillance videos. Most current methods build classifiers based on complex handcrafted features computed from the raw inputs. Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs. However, such models are currently limited to handling 2D inputs. In this paper, we develop a novel 3D CNN model for action recognition. This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels. To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models. We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods. {\textcopyright} 1979-2012 IEEE.},
author = {Ji, Shuiwang and Xu, Wei and Yang, Ming and Yu, Kai},
doi = {10.1109/TPAMI.2012.59},
file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ji et al. - Unknown - 3D convolutional neural networks for human action recognition.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {3D convolution,Deep learning,action recognition,convolutional neural networks,model combination},
number = {1},
pages = {221--231},
pmid = {22392705},
title = {{3D Convolutional neural networks for human action recognition}},
url = {https://ieeexplore.ieee.org/abstract/document/6165309/},
volume = {35},
year = {2013}
}
@inproceedings{Li2019,
abstract = {Automatically analyzing violent content in surveillance videos is of profound significance on many applications, ranging from Internet video filtration to public security protection. In this paper, we propose a deep learning model based on 3D convolutional neural networks, without using hand-crafted features or RNN architectures exclusively for encoding temporal information. The improved internal designs adopt compact but effective bottleneck units for learning motion patterns and leverage the DenseNet architecture to promote feature reusing and channel interaction, which is proved to be more capable of capturing spatiotemporal features and requires relatively fewer parameters. The performance of the proposed model is validated on three standard datasets in terms of recognition accuracy compared to other advanced approaches. Meanwhile, supplementary experiments are carried out to evaluate its effectiveness and efficiency. The final results demonstrate the advantages of the proposed model over the state-of-the-art methods in both recognition accuracy and computational efficiency.},
author = {Li, Ji and Jiang, Xinghao and Sun, Tanfeng and Xu, Ke},
booktitle = {2019 16th IEEE International Conference on Advanced Video and Signal Based Surveillance, AVSS 2019},
doi = {10.1109/AVSS.2019.8909883},
file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/C28_2019_AVSS_EfficientViolenceDetectionUsing3DConvolutionalNeuralNetworks.pdf:pdf},
isbn = {9781728109909},
month = {sep},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Efficient violence detection using 3D convolutional neural networks}},
year = {2019}
}
@inproceedings{Lillo2014,
abstract = {This paper proposes a framework for recognizing complex human activities in videos. Our method describes human activities in a hierarchical discriminative model that operates at three semantic levels. At the lower level, body poses are encoded in a representative but discriminative pose dictionary. At the intermediate level, encoded poses span a space where simple human actions are composed. At the highest level, our model captures temporal and spatial compositions of actions into complex human activities. Our human activity classifier simultaneously models which body parts are relevant to the action of interest as well as their appearance and composition using a discriminative approach. By formulating model learning in a max-margin framework, our approach achieves powerful multi-class discrimination while providing useful annotations at the intermediate semantic level. We show how our hierarchical compositional model provides natural handling of occlusions. To evaluate the effectiveness of our proposed framework, we introduce a new dataset of composed human activities. We provide empirical evidence that our method achieves state-of-the-art activity classification performance on several benchmark datasets.},
author = {Lillo, Ivan and Soto, Alvaro and Niebles, Juan Carlos},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.109},
file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lillo et al. - Unknown - Discriminative hierarchical modeling of spatio-temporally composable human activities.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
keywords = {action classification,composable actions,hierarchical modelling},
pages = {812--819},
title = {{Discriminative hierarchical modeling of spatio-temporally composable human activities}},
url = {http://openaccess.thecvf.com/content_cvpr_2014/html/Lillo_Discriminative_Hierarchical_Modeling_2014_CVPR_paper.html},
year = {2014}
}
@article{Mansimov2015,
abstract = {We propose a new way of incorporating temporal information present in videos into Spatial Convolutional Neural Networks (ConvNets) trained on images, that avoids training Spatio-Temporal ConvNets from scratch. We describe several initializations of weights in 3D Convolutional Layers of Spatio-Temporal ConvNet using 2D Convolutional Weights learned from ImageNet. We show that it is important to initialize 3D Convolutional Weights judiciously in order to learn temporal representations of videos. We evaluate our methods on the UCF-101 dataset and demonstrate improvement over Spatial ConvNets.},
archivePrefix = {arXiv},
arxivId = {1503.07274},
author = {Mansimov, Elman and Srivastava, Nitish and Salakhutdinov, Ruslan},
eprint = {1503.07274},
file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mansimov, Srivastava, Salakhutdinov - 2015 - Initialization Strategies of Spatio-Temporal Convolutional Neural Networks.pdf:pdf},
month = {mar},
title = {{Initialization Strategies of Spatio-Temporal Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1503.07274},
year = {2015}
}
@inproceedings{Morariu2011,
abstract = {We present a framework for the automatic recognition of complex multi-agent events in settings where structure is imposed by rules that agents must follow while performing activities. Given semantic spatio-temporal descriptions of what generally happens (i.e., rules, event descriptions, physical constraints), and based on video analysis, we determine the events that occurred. Knowledge about spatio-temporal structure is encoded using first-order logic using an approach based on Allen's Interval Logic, and robustness to low-level observation uncertainty is provided by Markov Logic Networks (MLN). Our main contribution is that we integrate interval-based temporal reasoning with probabilistic logical inference, relying on an efficient bottom-up grounding scheme to avoid combinatorial explosion. Applied to one-on-one basketball, our framework detects and tracks players, their hands and feet, and the ball, generates event observations from the resulting trajectories, and performs probabilistic logical inference to determine the most consistent sequence of events. We demonstrate our approach on 1hr (100,000 frames) of outdoor videos. {\textcopyright} 2011 IEEE.},
author = {Morariu, Vlad I. and Davis, Larry S.},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2011.5995386},
file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Morariu, 2011, 2011 - Unknown - Multi-agent event recognition in structured scenarios.pdf:pdf},
isbn = {9781457703942},
issn = {10636919},
pages = {3289--3296},
title = {{Multi-agent event recognition in structured scenarios}},
url = {https://ieeexplore.ieee.org/abstract/document/5995386/},
year = {2011}
}
@inproceedings{Ng2015,
abstract = {Convolutional neural networks (CNNs) have been extensively applied for image recognition problems giving state-of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted. We propose two methods capable of handling full length videos. The first method explores various convolutional temporal feature pooling architectures, examining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improvements over previously published results on the Sports 1 million dataset (73.1% vs. 60.9%) and the UCF-101 datasets with (88.6% vs. 88.0%) and without additional optical flow information (82.6% vs. 73.0%).},
archivePrefix = {arXiv},
arxivId = {1503.08909},
author = {Ng, Joe Yue Hei and Hausknecht, Matthew and Vijayanarasimhan, Sudheendra and Vinyals, Oriol and Monga, Rajat and Toderici, George},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7299101},
eprint = {1503.08909},
file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yue et al. - Unknown - Beyond short snippets Deep networks for video classification(2).pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
pages = {4694--4702},
title = {{Beyond short snippets: Deep networks for video classification}},
url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Ng_Beyond_Short_Snippets_2015_CVPR_paper.html},
volume = {07-12-June},
year = {2015}
}
@article{Peng2015,
	title={Encoding feature maps of cnns for action recognition},
	author={Peng, Xiaojiang and Schmid, Cordelia},
	year={2015}
}
@article{Robertson2006,
abstract = {In this paper we develop a system for human behaviour recognition in video sequences. Human behaviour is modelled as a stochastic sequence of actions. Actions are described by a feature vector comprising both trajectory information (position and velocity), and a set of local motion descriptors. Action recognition is achieved via probabilistic search of image feature databases representing previously seen actions. Hidden Markov Models (HMM) which encode scene rules are used to smooth sequences of actions. High-level behaviour recognition is achieved by computing the likelihood that a set of predefined HMMs explains the current action sequence. Thus, human actions and behaviour are represented using a hierarchy of abstraction: from person-centred actions, to actions with spatio-temporal context, to action sequences and, finally, general behaviours. While the upper levels all use Bayesian networks and belief propagation, the lowest level uses non-parametric sampling from a previously learned database of actions. The combined method represents a general framework for human behaviour modelling. We demonstrate results from broadcast tennis sequences and surveillance footage for automated video annotation. {\textcopyright} 2006 Elsevier Inc. All rights reserved.},
author = {Robertson, Neil and Reid, Ian},
doi = {10.1016/j.cviu.2006.07.006},
file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Robertson, Understanding, 2006 - Unknown - A general method for human activity recognition in video.pdf:pdf},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {Human activity recognition,Video annotation,Visual surveillance},
number = {2-3 SPEC. ISS.},
pages = {232--248},
title = {{A general method for human activity recognition in video}},
url = {https://www.sciencedirect.com/science/article/pii/S107731420600110X},
volume = {104},
year = {2006}
}
@inproceedings{Ryoo2011,
abstract = {In this paper, we present a novel approach of human activity prediction. Human activity prediction is a probabilistic process of inferring ongoing activities from videos only containing onsets (i.e. the beginning part) of the activities. The goal is to enable early recognition of unfinished activities as opposed to the after-the-fact classification of completed activities. Activity prediction methodologies are particularly necessary for surveillance systems which are required to prevent crimes and dangerous activities from occurring. We probabilistically formulate the activity prediction problem, and introduce new methodologies designed for the prediction. We represent an activity as an integral histogram of spatio-temporal features, efficiently modeling how feature distributions change over time. The new recognition methodology named dynamic bag-of-words is developed, which considers sequential nature of human activities while maintaining advantages of the bag-of-words to handle noisy observations. Our experiments confirm that our approach reliably recognizes ongoing activities from streaming videos with a high accuracy. {\textcopyright} 2011 IEEE.},
author = {Ryoo, M. S.},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126349},
file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/vision, 2011 - 2011 - Human activity prediction Early recognition of ongoing activities from streaming videos.pdf:pdf},
isbn = {9781457711015},
pages = {1036--1043},
title = {{Human activity prediction: Early recognition of ongoing activities from streaming videos}},
url = {https://ieeexplore.ieee.org/abstract/document/6126349/},
year = {2011}
}
@inproceedings{Schuldt2004,
	title={Recognizing human actions: a local SVM approach},
	author={Schuldt, Christian and Laptev, Ivan and Caputo, Barbara},
	booktitle={Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.},
	volume={3},
	pages={32--36},
	year={2004},
	organization={IEEE}
}
@inproceedings{Simonyan2014,
abstract = {We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multitask learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.},
archivePrefix = {arXiv},
arxivId = {1406.2199},
author = {Simonyan, Karen and Zisserman, Andrew},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1406.2199},
file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, information, 2014 - Unknown - Two-stream convolutional networks for action recognition in videos.pdf:pdf},
issn = {10495258},
number = {January},
pages = {568--576},
title = {{Two-stream convolutional networks for action recognition in videos}},
url = {https://proceedings.neurips.cc/paper_files/paper/2014/hash/00ec53c4682d36f5c4359f4ae7bd7ba1-Abstract.html},
volume = {1},
year = {2014}
}
@inproceedings{Singh2017,
abstract = {Human activity recognition using smart home sensors is one of the bases of ubiquitous computing in smart environments and a topic undergoing intense research in the field of ambient assisted living. The increasingly large amount of data sets calls for machine learning methods. In this paper, we introduce a deep learning model that learns to classify human activities without using any prior knowledge. For this purpose, a Long Short Term Memory (LSTM) Recurrent Neural Network was applied to three real world smart home datasets. The results of these experiments show that the proposed approach outperforms the existing ones in terms of accuracy and performance.},
archivePrefix = {arXiv},
arxivId = {1804.07144},
author = {Singh, Deepika and Merdivan, Erinc and Psychoula, Ismini and Kropf, Johannes and Hanke, Sten and Geist, Matthieu and Holzinger, Andreas},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-66808-6_18},
eprint = {1804.07144},
file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Singh et al. - Unknown - Human activity recognition using recurrent neural networks.pdf:pdf},
isbn = {9783319668079},
issn = {16113349},
keywords = {Ambient assisted living,Deep learning,Human activity recognition,LSTM,Machine learning,Sensors},
pages = {267--274},
title = {{Human activity recognition using recurrent neural networks}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-66808-6_18},
volume = {10410 LNCS},
year = {2017}
}
@inproceedings{Song2013,
abstract = {Recent progress has shown that learning from hierarchical feature representations leads to improvements in various computer vision tasks. Motivated by the observation that human activity data contains information at various temporal resolutions, we present a hierarchical sequence summarization approach for action recognition that learns multiple layers of discriminative feature representations at different temporal granularities. We build up a hierarchy dynamically and recursively by alternating sequence learning and sequence summarization. For sequence learning we use CRFs with latent variables to learn hidden spatio-temporal dynamics, for sequence summarization we group observations that have similar semantic meaning in the latent space. For each layer we learn an abstract feature representation through non-linear gate functions. This procedure is repeated to obtain a hierarchical sequence summary representation. We develop an efficient learning method to train our model and show that its complexity grows sub linearly with the size of the hierarchy. Experimental results show the effectiveness of our approach, achieving the best published results on the Arm Gesture and Canal9 datasets. {\textcopyright} 2013 IEEE.},
author = {Song, Yale and Morency, Louis Philippe and Davis, Randall},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2013.457},
file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Song et al. - 2013 - Action recognition by hierarchical sequence summarization.pdf:pdf},
issn = {10636919},
keywords = {Action Recognition,Conditional Random Fields,Hierarchical Model},
pages = {3562--3569},
title = {{Action recognition by hierarchical sequence summarization}},
url = {http://openaccess.thecvf.com/content_cvpr_2013/html/Song_Action_Recognition_by_2013_CVPR_paper.html},
year = {2013}
}
@inproceedings{Tran2015,
abstract = {We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets, 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets, and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8% accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use.},
archivePrefix = {arXiv},
arxivId = {1412.0767},
author = {Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.510},
eprint = {1412.0767},
file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tran et al. - Unknown - Learning spatiotemporal features with 3d convolutional networks.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
pages = {4489--4497},
title = {{Learning spatiotemporal features with 3D convolutional networks}},
url = {http://openaccess.thecvf.com/content_iccv_2015/html/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.html},
volume = {2015 Inter},
year = {2015}
}
@article{Vrigkas2015,
	title={A review of human activity recognition methods},
	author={Vrigkas, Michalis and Nikou, Christophoros and Kakadiaris, Ioannis A},
	journal={Frontiers in Robotics and AI},
	volume={2},
	pages={28},
	year={2015},
	publisher={Frontiers Media SA}
}
@article{Vrigkas2014matching,
	title={Matching mixtures of curves for human action recognition},
	author={Vrigkas, Michalis and Karavasilis, Vasileios and Nikou, Christophoros and Kakadiaris, Ioannis A},
	journal={Computer Vision and Image Understanding},
	volume={119},
	pages={27--40},
	year={2014},
	publisher={Elsevier}
}
@article{wang2010hidden,
	title={Hidden part models for human action recognition: Probabilistic versus max margin},
	author={Wang, Yang and Mori, Greg},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume={33},
	number={7},
	pages={1310--1323},
	year={2010},
	publisher={IEEE}
}
@inproceedings{Wang2013,
abstract = {Recently dense trajectories were shown to be an efficient video representation for action recognition and achieved state-of-the-art results on a variety of datasets. This paper improves their performance by taking into account camera motion to correct them. To estimate camera motion, we match feature points between frames using SURF descriptors and dense optical flow, which are shown to be complementary. These matches are, then, used to robustly estimate a homography with RANSAC. Human motion is in general different from camera motion and generates inconsistent matches. To improve the estimation, a human detector is employed to remove these matches. Given the estimated camera motion, we remove trajectories consistent with it. We also use this estimation to cancel out camera motion from the optical flow. This significantly improves motion-based descriptors, such as HOF and MBH. Experimental results on four challenging action datasets (i.e., Hollywood2, HMDB51, Olympic Sports and UCF50) significantly outperform the current state of the art. {\textcopyright} 2013 IEEE.},
author = {Wang, Heng and Schmid, Cordelia},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2013.441},
isbn = {9781479928392},
pages = {3551--3558},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Action recognition with improved trajectories}},
year = {2013}
}
@article{Yadav2021,
abstract = {Human activity recognition (HAR) is one of the most important and challenging problems in the computer vision. It has critical application in wide variety of tasks including gaming, humanâ€“robot interaction, rehabilitation, sports, health monitoring, video surveillance, and robotics. HAR is challenging due to the complex posture made by the human and multiple people interaction. Various artifacts that commonly appears in the scene such as illuminations variations, clutter, occlusions, background diversity further adds the complexity to HAR. Sensors for multiple modalities could be used to overcome some of these inherent challenges. Such sensors could include an RGB-D camera, infrared sensors, thermal cameras, inertial sensors, etc. This article introduces a comprehensive review of different multimodal human activity recognition methods where different types of sensors being used along with their analytical approaches and fusion methods. Further, this article presents classification and discussion of existing work within seven rational aspects: (a) what are the applications of HAR; (b) what are the single and multi-modality sensing for HAR; (c) what are different vision based approaches for HAR; (d) what and how wearable sensors based system contributes to the HAR; (e) what are different multimodal HAR methods; (f) how a combination of vision and wearable inertial sensors based system contributes to the HAR; and (g) challenges and future directions in HAR. With a more and comprehensive understanding of multimodal human activity recognition, more research in this direction can be motivated and refined.},
author = {Yadav, Santosh Kumar and Tiwari, Kamlesh and Pandey, Hari Mohan and Akbar, Shaik Ali},
doi = {10.1016/j.knosys.2021.106970},
file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yadav et al. - Unknown - A review of multimodal human activity recognition with special emphasis on classification, applications, challe.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Activity recognition,Computer vision,Fusion of vision and inertial sensors,Multimodality,Smart-shoes,Wearable sensors},
title = {{A review of multimodal human activity recognition with special emphasis on classification, applications, challenges and future directions}},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121002331},
volume = {223},
year = {2021}
}
@article{Yan2012,
abstract = {The bag of interest points (BIPs) approach is a good strategy for human action recognition, but it ignores much information contained in the spatial-temporal interest points (STIPs), while the lost information is helpful for classification. In this paper, a new action descriptor based on the STIPs is proposed: histogram of interest point locations (HIPLs). HIPL reorganizes STIPs and reflects the spatial location information, and can be viewed as a useful supplement to the BIP feature. Multiple features including BIP and HIPL are extracted to describe human actions, however, it leads to over-fitting easily by combining them directly because the dimension of feature vector is too high. To overcome this problem, a novel classifier combination framework is developed to integrate the multiple features, and AdaBoost and sparse representation (SR) are used as basic algorithms. Experiments on KTH and UCF sports datasets which are two benchmarks in human action recognition, show that our results are either comparable to, or significantly better than previously published results on these benchmarks. {\textcopyright} 2012 Elsevier B.V.},
author = {Yan, Xunshi and Luo, Yupin},
doi = {10.1016/j.neucom.2012.02.002},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Action recognition,AdaBoost,Classifier combination,Sparse representation,Spatial-temporal interest points},
pages = {51--61},
title = {{Recognizing human actions using a new descriptor based on spatial-temporal interest points and weighted-output classifier}},
url = {https://www.sciencedirect.com/science/article/pii/S0925231212000677},
volume = {87},
year = {2012}
}
@inproceedings{Yang2010,
abstract = {We consider the problem of recognizing human actions from still images. We propose a novel approach that treats the pose of the person in the image as latent variables that will help with recognition. Different from other work that learns separate systems for pose estimation and action recognition, then combines them in an ad-hoc fashion, our system is trained in an integrated fashion that jointly considers poses and actions. Our learning objective is designed to directly exploit the pose information for action recognition. Our experimental results demonstrate that by inferring the latent poses, we can improve the final action recognition results. {\textcopyright}2010 IEEE.},
author = {Yang, Weilong and Wang, Yang and Mori, Greg},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2010.5539879},
file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - Unknown - Recognizing human actions from still images with latent poses.pdf:pdf},
isbn = {9781424469840},
issn = {10636919},
pages = {2030--2037},
title = {{Recognizing human actions from still images with latent poses}},
url = {https://ieeexplore.ieee.org/abstract/document/5539879/},
year = {2010}
}
@article{Zhang2018,
abstract = {The two-stream CNNs prove very successful for video-based action recognition. However, the classical two-stream CNNs are time costly, mainly due to the bottleneck of calculating optical flows (OFs). In this paper, we propose a two-stream-based real-time action recognition approach by using motion vector (MV) to replace OF. MVs are encoded in video stream and can be extracted directly without extra calculation. However, directly training CNN with MVs degrades accuracy severely due to the noise and the lack of fine details in MVs. In order to relieve this problem, we propose four training strategies which leverage the knowledge learned from OF CNN to enhance the accuracy of MV CNN. Our insight is that MV and OF share inherent similar structures which allow us to transfer knowledge from one domain to another. To fully utilize the knowledge learned in OF domain, we develop deeply transferred MV CNN. Experimental results on various datasets show the effectiveness of our training strategies. Our approach is significantly faster than OF based approaches and achieves processing speed of 390.7 frames per second, surpassing real-time requirement. We release our model and code to facilitate further research.11https://github.com/zbwglory/MV-release.},
author = {Zhang, Bowen and Wang, Limin and Wang, Zhe and Qiao, Yu and Wang, Hanli},
doi = {10.1109/TIP.2018.2791180},
file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - Unknown - Real-time action recognition with enhanced motion vector CNNs.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Action recognition,deep learning,knowledge transfer,motion vector,real-time processing},
number = {5},
pages = {2326--2339},
title = {{Real-Time Action Recognition with Deeply Transferred Motion Vector CNNs}},
url = {http://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Real-Time_Action_Recognition_CVPR_2016_paper.html},
volume = {27},
year = {2018}
}
