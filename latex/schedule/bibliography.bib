@inproceedings{Ann2014,
abstract = {Human Activity Recognition is one of the active research areas in computer vision for various contexts like security surveillance, healthcare and human computer interaction. In this paper, a total of thirty-two recent research papers on sensing technologies used in HAR are reviewed. The review covers three area of sensing technologies namely RGB cameras, depth sensors and wearable devices. It also discusses on the pros and cons of the mentioned sensing technologies. The findings showed that RGB cameras have lower popularity when compared to depth sensors and wearable devices in HAR research.},
author = {Ann, Ong Chin and Theng, Lau Bee},
booktitle = {Proceedings - 4th IEEE International Conference on Control System, Computing and Engineering, ICCSCE},
doi = {10.1109/ICCSCE.2014.7072750},
file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ann, on, 2014 - Unknown - Human activity recognition A review.pdf:pdf},
isbn = {9781479956869},
keywords = {Human activity recognition,Kinect,RGB camera,depth sensor,sensing technology,wearable devices},
pages = {389--393},
title = {{Human activity recognition: A review}},
url = {https://ieeexplore.ieee.org/abstract/document/7072750/},
year = {2014}
}
@inproceedings{Asadi-Aghbolaghi2017,
	abstract = {The interest in action and gesture recognition has grown considerably in the last years. In this paper, we present a survey on current deep learning methodologies for action and gesture recognition in image sequences. We introduce a taxonomy that summarizes important aspects of deep learning for approaching both tasks. We review the details of the proposed architectures, fusion strategies, main datasets, and competitions. We summarize and discuss the main works proposed so far with particular interest on how they treat the temporal dimension of data, discussing their main features and identify opportunities and challenges for future research.},
	author = {Asadi-Aghbolaghi, Maryam and Clapes, Albert and Bellantonio, Marco and Escalante, Hugo Jair and Ponce-Lopez, Victor and Baro, Xavier and Guyon, Isabelle and Kasaei, Shohreh and Escalera, Sergio},
	booktitle = {Proceedings - 12th IEEE International Conference on Automatic Face and Gesture Recognition, FG - 1st International Workshop on Adaptive Shot Learning for Gesture Understanding and Production, ASL4GUP, Biometrics in the Wild, Bwild, Heteroge},
	doi = {10.1109/FG.2017.150},
	file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Asadi-Aghbolaghi et al. - Unknown - A survey on deep learning based approaches for action and gesture recognition in image sequences.pdf:pdf},
	isbn = {9781509040230},
	pages = {476--483},
	title = {{A Survey on Deep Learning Based Approaches for Action and Gesture Recognition in Image Sequences}},
	url = {https://ieeexplore.ieee.org/abstract/document/7961779/},
	year = {2017}
}
@inproceedings{Feichtenhofer2016,
	abstract = {Recent applications of Convolutional Neural Networks (ConvNets) for human action recognition in videos have proposed different solutions for incorporating the appearance and motion information. We study a number of ways of fusing ConvNet towers both spatially and temporally in order to best take advantage of this spatio-temporal information. We make the following findings: (i) that rather than fusing at the softmax layer, a spatial and temporal network can be fused at a convolution layer without loss of performance, but with a substantial saving in parameters, (ii) that it is better to fuse such networks spatially at the last convolutional layer than earlier, and that additionally fusing at the class prediction layer can boost accuracy, finally (iii) that pooling of abstract convolutional features over spatiotemporal neighbourhoods further boosts performance. Based on these studies we propose a new ConvNet architecture for spatiotemporal fusion of video snippets, and evaluate its performance on standard benchmarks where this architecture achieves state-of-the-art results.},
	archivePrefix = {arXiv},
	arxivId = {1604.06573},
	author = {Feichtenhofer, Christoph and Pinz, Axel and Zisserman, Andrew},
	booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	doi = {10.1109/CVPR.2016.213},
	eprint = {1604.06573},
	file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Feichtenhofer, {\ldots}, 2016 - Unknown - Convolutional two-stream network fusion for video action recognition.pdf:pdf},
	isbn = {9781467388504},
	issn = {10636919},
	mendeley-groups = {Tesis - HAR/RSL - Proyectos,Tesis - HAR/RSL - Primarios},
	pages = {1933--1941},
	title = {{Convolutional Two-Stream Network Fusion for Video Action Recognition}},
	url = {http://openaccess.thecvf.com/content_cvpr_2016/html/Feichtenhofer_Convolutional_Two-Stream_Network_CVPR_2016_paper.html},
	volume = {December},
	year = {2016}
}
@inproceedings{Fernandes2022,
	author={Fernandes, Cristiana and Matos, Luís Miguel and Folgado, Duarte and Nunes, Maria Lua and Pereira, João Rui and Pilastri, André and Cortez, Paulo},
	booktitle={International Joint Conference on Neural Networks (IJCNN)}, 
	title={A Deep Learning Approach to Prevent Problematic Movements of Industrial Workers Based on Inertial Sensors}, 
	year={2022},
	volume={},
	number={},
	pages={01-08},
	doi={10.1109/IJCNN55064.2022.9892409}
}
@inproceedings{Gerling2012,
	abstract = {Older adults in nursing homes often lead sedentary lifestyles, which reduces their life expectancy. Full-body motion-control games provide an opportunity for these adults to remain active and engaged; these games are not designed with age-related impairments in mind, which prevents the games from being leveraged to increase the activity levels of older adults. In this paper, we present two studies aimed at developing game design guidelines for full-body motion controls for older adults experiencing age-related changes and impairments. Our studies also demonstrate how full-body motion-control games can accommodate a variety of user abilities, have a positive effect on mood and, by extension, the emotional well-being of older adults. Based on our studies, we present seven guidelines for the design of full-body interaction in games. The guidelines are designed to foster safe physical activity among older adults, thereby increasing their quality of life. Copyright 2012 ACM.},
	author = {Gerling, Kathrin M. and Livingston, Ian J. and Nacke, Lennart E. and Mandryk, Regan L.},
	booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
	doi = {10.1145/2207676.2208324},
	file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gerling et al. - 2012 - Full-body motion-based game interaction for older adults.pdf:pdf},
	isbn = {9781450310154},
	keywords = {Design,Entertainment,Games,Older adults},
	pages = {1873--1882},
	title = {{Full-body motion-based game interaction for older adults}},
	url = {https://dl.acm.org/doi/abs/10.1145/2207676.2208324},
	year = {2012}
}
@article{Gonzalez-Ortega2014,
	abstract = {In this paper, a 3D computer vision system for cognitive assessment and rehabilitation based on the Kinect device is presented. It is intended for individuals with body scheme dysfunctions and left-right confusion. The system processes depth information to overcome the shortcomings of a previously presented 2D vision system for the same application. It achieves left and right-hand tracking, and face and facial feature detection (eye, nose, and ears) detection. The system is easily implemented with a consumer-grade computer and an affordable Kinect device and is robust to drastic background and illumination changes. The system was tested and achieved a successful monitoring percentage of 96.28%. The automation of the human body parts motion monitoring, its analysis in relation to the psychomotor exercise indicated to the patient, and the storage of the result of the realization of a set of exercises free the rehabilitation experts of doing such demanding tasks. The vision-based system is potentially applicable to other tasks with minor changes. {\textcopyright} 2013 Elsevier Ireland Ltd.},
	author = {Gonz{\'{a}}lez-Ortega, D and D{\'{i}}az-Pernas, F J and Mart{\'{i}}nez-Zarzuela, M and Ant{\'{o}}n-Rodr{\'{i}}guez, M},
	doi = {10.1016/j.cmpb.2013.10.014},
	file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gonz{\'{a}}lez-Ortega et al. - 2013 - A Kinect-based system for cognitive rehabilitation exercises monitoring.pdf:pdf},
	issn = {01692607},
	journal = {Computer Methods and Programs in Biomedicine},
	keywords = {3D computer vision,Cognitive rehabilitation,Human body parts detection and tracking,Human-computer interaction,Kinect device},
	number = {2},
	pages = {620--631},
	pmid = {24263055},
	title = {{A Kinect-based system for cognitive rehabilitation exercises monitoring}},
	url = {https://www.sciencedirect.com/science/article/pii/S0169260713003568},
	volume = {113},
	year = {2014}
}
@article{Ji2013,
	abstract = {We consider the automated recognition of human actions in surveillance videos. Most current methods build classifiers based on complex handcrafted features computed from the raw inputs. Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs. However, such models are currently limited to handling 2D inputs. In this paper, we develop a novel 3D CNN model for action recognition. This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels. To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models. We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods. {\textcopyright} 1979-2012 IEEE.},
	author = {Ji, Shuiwang and Xu, Wei and Yang, Ming and Yu, Kai},
	doi = {10.1109/TPAMI.2012.59},
	file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ji et al. - Unknown - 3D convolutional neural networks for human action recognition(2).pdf:pdf},
	issn = {01628828},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {3D convolution,Deep learning,action recognition,convolutional neural networks,model combination},
	mendeley-groups = {Tesis - HAR/RSL - Proyectos,Tesis - HAR/RSL - Primarios},
	number = {1},
	pages = {221--231},
	pmid = {22392705},
	title = {{3D Convolutional neural networks for human action recognition}},
	url = {https://ieeexplore.ieee.org/abstract/document/6165309/},
	volume = {35},
	year = {2013}
}
@inproceedings{Krizhevsky2017,
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	title = {ImageNet Classification with Deep Convolutional Neural Networks},
	year = {2012},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
	pages = {1097–1105},
	numpages = {9},
	location = {Lake Tahoe, Nevada},
	series = {NIPS'12}
}
@inproceedings{Li2016,
	title={Performance analysis of GPU-based convolutional neural networks},
	author={Li, Xiaqing and Zhang, Guangyan and Huang, H Howie and Wang, Zhufan and Zheng, Weimin},
	booktitle={45th International conference on parallel processing (ICPP)},
	pages={67--76},
	year={2016},
	organization={IEEE}
}
@inproceedings{Li2019,
	abstract = {Automatically analyzing violent content in surveillance videos is of profound significance on many applications, ranging from Internet video filtration to public security protection. In this paper, we propose a deep learning model based on 3D convolutional neural networks, without using hand-crafted features or RNN architectures exclusively for encoding temporal information. The improved internal designs adopt compact but effective bottleneck units for learning motion patterns and leverage the DenseNet architecture to promote feature reusing and channel interaction, which is proved to be more capable of capturing spatiotemporal features and requires relatively fewer parameters. The performance of the proposed model is validated on three standard datasets in terms of recognition accuracy compared to other advanced approaches. Meanwhile, supplementary experiments are carried out to evaluate its effectiveness and efficiency. The final results demonstrate the advantages of the proposed model over the state-of-the-art methods in both recognition accuracy and computational efficiency.},
	author = {Li, Ji and Jiang, Xinghao and Sun, Tanfeng and Xu, Ke},
	booktitle = {16th IEEE International Conference on Advanced Video and Signal Based Surveillance, AVSS},
	doi = {10.1109/AVSS.2019.8909883},
	file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/C28_2019_AVSS_EfficientViolenceDetectionUsing3DConvolutionalNeuralNetworks.pdf:pdf},
	isbn = {9781728109909},
	month = {sep},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	title = {{Efficient violence detection using 3D convolutional neural networks}},
	year = {2019}
}
@article{lu2020,
	title={Driver action recognition using deformable and dilated faster R-CNN with optimized region proposals},
	author={Lu, Mingqi and Hu, Yaocong and Lu, Xiaobo},
	journal={Applied Intelligence},
	volume={50},
	pages={1100--1111},
	year={2020},
	publisher={Springer}
}
@article{Mohamed2022,
	author={Mohamed, Mai and El-Kilany, Ayman and El-Tazi, Neamat},
	journal={IEEE Access}, 
	title={Future Activities Prediction Framework in Smart Homes Environment}, 
	year={2022},
	volume={10},
	number={},
	pages={85154-85169},
	doi={10.1109/ACCESS.2022.3197618}
}
@inproceedings{Ryoo2011,
	abstract = {In this paper, we present a novel approach of human activity prediction. Human activity prediction is a probabilistic process of inferring ongoing activities from videos only containing onsets (i.e. the beginning part) of the activities. The goal is to enable early recognition of unfinished activities as opposed to the after-the-fact classification of completed activities. Activity prediction methodologies are particularly necessary for surveillance systems which are required to prevent crimes and dangerous activities from occurring. We probabilistically formulate the activity prediction problem, and introduce new methodologies designed for the prediction. We represent an activity as an integral histogram of spatio-temporal features, efficiently modeling how feature distributions change over time. The new recognition methodology named dynamic bag-of-words is developed, which considers sequential nature of human activities while maintaining advantages of the bag-of-words to handle noisy observations. Our experiments confirm that our approach reliably recognizes ongoing activities from streaming videos with a high accuracy. {\textcopyright} 2011 IEEE.},
	author = {Ryoo, M. S.},
	booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
	doi = {10.1109/ICCV.2011.6126349},
	file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/vision, 2011 - 2011 - Human activity prediction Early recognition of ongoing activities from streaming videos.pdf:pdf},
	isbn = {9781457711015},
	pages = {1036--1043},
	title = {{Human activity prediction: Early recognition of ongoing activities from streaming videos}},
	url = {https://ieeexplore.ieee.org/abstract/document/6126349/},
	year = {2011}
}
@inproceedings{Tan2019,
	title={Efficientnet: Rethinking model scaling for convolutional neural networks},
	author={Tan, Mingxing and Le, Quoc},
	booktitle={International conference on machine learning},
	pages={6105--6114},
	year={2019},
	organization={PMLR}
}
@article{Ullah2017,
	abstract = {Recurrent neural network (RNN) and long short-Term memory (LSTM) have achieved great success in processing sequential multimedia data and yielded the state-of-The-Art results in speech recognition, digital signal processing, video processing, and text data analysis. In this paper, we propose a novel action recognition method by processing the video data using convolutional neural network (CNN) and deep bidirectional LSTM (DB-LSTM) network. First, deep features are extracted from every sixth frame of the videos, which helps reduce the redundancy and complexity. Next, the sequential information among frame features is learnt using DB-LSTM network, where multiple layers are stacked together in both forward pass and backward pass of DB-LSTM to increase its depth. The proposed method is capable of learning long term sequences and can process lengthy videos by analyzing features for a certain time interval. Experimental results show significant improvements in action recognition using the proposed method on three benchmark data sets including UCF-101, YouTube 11 Actions, and HMDB51 compared with the state-of-The-Art action recognition methods.},
	author = {Ullah, Amin and Ahmad, Jamil and Muhammad, Khan and Sajjad, Muhammad and Baik, Sung Wook},
	doi = {10.1109/ACCESS.2017.2778011},
	file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ullah et al. - Unknown - Action recognition in video sequences using deep bi-directional LSTM with CNN features.pdf:pdf},
	issn = {21693536},
	journal = {IEEE Access},
	keywords = {Action recognition,convolution neural network,deep bidirectional long short-Term memory,deep learning,recurrent neural network},
	mendeley-groups = {Tesis - HAR/RSL - Proyectos,Tesis - HAR/RSL - Primarios},
	pages = {1155--1166},
	title = {{Action Recognition in Video Sequences using Deep Bi-Directional LSTM with CNN Features}},
	url = {https://ieeexplore.ieee.org/abstract/document/8121994/},
	volume = {6},
	year = {2017}
}
@article{Varol2018,
	abstract = {Typical human actions last several seconds and exhibit characteristic spatio-temporal structure. Recent methods attempt to capture this structure and learn action representations with convolutional neural networks. Such representations, however, are typically learned at the level of a few video frames failing to model actions at their full temporal extent. In this work we learn video representations using neural networks with long-term temporal convolutions (LTC). We demonstrate that LTC-CNN models with increased temporal extents improve the accuracy of action recognition. We also study the impact of different low-level representations, such as raw values of video pixels and optical flow vector fields and demonstrate the importance of high-quality optical flow estimation for learning accurate action models. We report state-of-the-art results on two challenging benchmarks for human action recognition UCF101 (92.7%) and HMDB51 (67.2%).},
	archivePrefix = {arXiv},
	arxivId = {1604.04494},
	author = {Varol, Gul and Laptev, Ivan and Schmid, Cordelia},
	doi = {10.1109/TPAMI.2017.2712608},
	eprint = {1604.04494},
	file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Varol et al. - Unknown - Long-term temporal convolutions for action recognition.pdf:pdf},
	isbn = {1604.04494v2},
	issn = {01628828},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Action recognition,neural networks,representation learning,spatio-temporal convolutions,video analysis},
	mendeley-groups = {Tesis - HAR/RSL - Proyectos,Tesis - HAR/RSL - Primarios},
	number = {6},
	pages = {1510--1517},
	pmid = {28600238},
	title = {{Long-Term Temporal Convolutions for Action Recognition}},
	url = {https://ieeexplore.ieee.org/abstract/document/7940083/},
	volume = {40},
	year = {2018}
}
@inproceedings{Zhu2019,
	abstract = {Analyzing videos of human actions involves understanding the temporal relationships among video frames. State-of-the-art action recognition approaches rely on traditional optical flow estimation methods to pre-compute motion information for CNNs. Such a two-stage approach is computationally expensive, storage demanding, and not end-to-end trainable. In this paper, we present a novel CNN architecture that implicitly captures motion information between adjacent frames. We name our approach hidden two-stream CNNs because it only takes raw video frames as input and directly predicts action classes without explicitly computing optical flow. Our end-to-end approach is 10x faster than its two-stage baseline. Experimental results on four challenging action recognition datasets: UCF101, HMDB51, THUMOS14 and ActivityNet v1.2 show that our approach significantly outperforms the previous best real-time approaches.},
	archivePrefix = {arXiv},
	arxivId = {1704.00389},
	author = {Zhu, Yi and Lan, Zhenzhong and Newsam, Shawn and Hauptmann, Alexander},
	booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	doi = {10.1007/978-3-030-20893-6_23},
	eprint = {1704.00389},
	file = {:C\:/Users/rfondato/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu et al. - Unknown - Hidden two-stream convolutional networks for action recognition.pdf:pdf},
	isbn = {9783030208929},
	issn = {16113349},
	keywords = {Action recognition,Optical flow,Unsupervised learning},
	mendeley-groups = {Tesis - HAR/RSL - Proyectos,Tesis - HAR/RSL - Primarios},
	pages = {363--378},
	title = {{Hidden Two-Stream Convolutional Networks for Action Recognition}},
	url = {https://link.springer.com/chapter/10.1007/978-3-030-20893-6_23},
	volume = {11363 LNCS},
	year = {2019}
}

